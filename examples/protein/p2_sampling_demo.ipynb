{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2 Sampling Demo for Protein Sequence Generation\n",
    "\n",
    "This notebook demonstrates how to use P2 (Path Planning) sampling to generate protein sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "from transformers import AutoTokenizer, EsmForMaskedLM\n",
    "from path_planning.p2 import p2_sampling\n",
    "from path_planning.utils import seed_everything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define some helper functions for our demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_special_tokens_logits(logits, tokenizer):\n",
    "    \"\"\"Masks out the logits of special tokens to prevent them from being sampled.\"\"\"\n",
    "    logits[..., tokenizer.mask_token_id] = -math.inf\n",
    "    logits[..., tokenizer._token_to_id[\"X\"]] = -math.inf\n",
    "    logits[..., tokenizer.pad_token_id] = -math.inf\n",
    "    logits[..., tokenizer.cls_token_id] = -math.inf\n",
    "    logits[..., tokenizer.eos_token_id] = -math.inf\n",
    "    return logits\n",
    "\n",
    "class ModelWrapper:\n",
    "    \"\"\"Wrapper for the ESM model to handle logits processing.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        outputs = self.model(x)\n",
    "        logits = outputs.logits\n",
    "        return ignore_special_tokens_logits(logits.float(), self.tokenizer)\n",
    "\n",
    "def create_masked_sequence(sequence_length, tokenizer, batch_size=1, device='cuda'):\n",
    "    \"\"\"Create a fully masked sequence for generation.\"\"\"\n",
    "    seq = [tokenizer.mask_token] * sequence_length\n",
    "    sequences = [''.join(seq)] * batch_size\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        sequences,\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the parameters for protein sequence generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"airkingbd/dplm_650m\"  # You can also try \"zhangzhi/EvoFlow-150M-fs\"\n",
    "num_seqs = 5  # Number of sequences to generate\n",
    "seq_len = 100  # Length of sequences\n",
    "num_steps = 100  # Number of P2 sampling steps\n",
    "temperature = 1.0  # Sampling temperature\n",
    "eta = 1.0  # Stochasticity strength\n",
    "seed = 42  # Random seed\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the protein language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "model = model.eval().to(device)\n",
    "\n",
    "# Wrap the model\n",
    "model_wrapper = ModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Initial Sequence\n",
    "\n",
    "Create a fully masked sequence as the starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating initial sequence...\")\n",
    "xt = create_masked_sequence(\n",
    "    sequence_length=seq_len,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=num_seqs,\n",
    "    device=device\n",
    ")\n",
    "print(f\"Initial sequence shape: {xt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run P2 Sampling\n",
    "\n",
    "Generate protein sequences using P2 sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting P2 sampling...\")\n",
    "start_time = time.time()\n",
    "# check out p2_sampling to see the full parameters\n",
    "sampled_xt = p2_sampling(\n",
    "    xt=xt,\n",
    "    model=model_wrapper,\n",
    "    tokenizer=tokenizer,\n",
    "    num_steps=num_steps,\n",
    "    tau=temperature,\n",
    "    eta=eta\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Generation completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Tokens/second: {num_seqs * seq_len / elapsed_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode and Display Results\n",
    "\n",
    "Decode the generated sequences and display them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sequences\n",
    "decoded_seqs = tokenizer.batch_decode(sampled_xt, skip_special_tokens=True)\n",
    "decoded_seqs = [''.join(seq.split()) for seq in decoded_seqs]\n",
    "\n",
    "# Display generated sequences\n",
    "print(\"\\nGenerated Protein Sequences:\")\n",
    "for i, seq in enumerate(decoded_seqs):\n",
    "    print(f\"Sequence {i+1} (length {len(seq)}):\")\n",
    "    print(seq)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Sequences (Optional)\n",
    "\n",
    "Save the generated sequences to a FASTA file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sequences_to_fasta(sequences, seq_len, save_path):\n",
    "    \"\"\"Save generated sequences to FASTA format.\"\"\"\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as fp:\n",
    "        for idx, seq in enumerate(sequences):\n",
    "            fp.write(f\">SEQUENCE_{idx}_L={seq_len}\\n\")\n",
    "            fp.write(f\"{seq}\\n\")\n",
    "\n",
    "# Uncomment to save sequences\n",
    "# save_path = \"generated_sequences.fasta\"\n",
    "# save_sequences_to_fasta(decoded_seqs, seq_len, save_path)\n",
    "# print(f\"Saved sequences to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
